{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69c3d0d8",
   "metadata": {},
   "source": [
    "# Micropillar Dual-Mode Image Analysis Pipeline (Pillar U-Net + C2 Quantification)\n",
    "\n",
    "This single notebook is intended for **public reproducibility** (e.g., a GitHub repo for reviewers).\n",
    "It:\n",
    "\n",
    "1. **Downloads** the trained pillar-segmentation model from Box (if missing)\n",
    "2. Runs **U-Net inference** on **C1** images to create pillar masks\n",
    "3. Uses pillar masks to quantify **birefringent CaCO₃** signal in **C2** images\n",
    "4. Exports **per-image metrics** and **condition-aggregated summaries** as CSV\n",
    "\n",
    "**Expected naming convention (recommended):**  \n",
    "`<channel>.<section>.<diameter>.<trial>.c1.png` and `...c2.png`  \n",
    "Example: `t1.1.8.a.c1.png` and `t1.1.8.a.c2.png`\n",
    "\n",
    "If your filenames differ, update `parse_filename()` and `pair_c1_c2()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a34c0c",
   "metadata": {},
   "source": [
    "### Optional: install dependencies (Colab)\n",
    "\n",
    "If you're running in Google Colab, uncomment and run the install cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2efde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab / fresh environments)\n",
    "!pip -q install segmentation-models-pytorch albumentations opencv-python tqdm pandas matplotlib requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d3df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 0) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_dir(p: Path) -> Path:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def download_box_file(url: str, out_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Downloads a file from a Box shared link.\n",
    "\n",
    "    Note: most Box share links can be downloaded by appending '?download=1'\n",
    "    \"\"\"\n",
    "    import requests\n",
    "\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"content-length\", 0))\n",
    "        with open(out_path, \"wb\") as f:\n",
    "            pbar = tqdm(total=total, unit=\"B\", unit_scale=True, desc=f\"Downloading {out_path.name}\")\n",
    "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "            pbar.close()\n",
    "\n",
    "def read_grayscale_uint8(img_path: Path) -> np.ndarray:\n",
    "    \"\"\"Reads an image as uint8 grayscale (0..255).\"\"\"\n",
    "    img = cv2.imread(str(img_path), cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Could not read image: {img_path}\")\n",
    "    if img.ndim == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # If 16-bit, scale to 8-bit for consistent thresholding\n",
    "    if img.dtype == np.uint16:\n",
    "        img = (img / 256).astype(np.uint8)\n",
    "    elif img.dtype != np.uint8:\n",
    "        img = cv2.normalize(img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def save_mask_png(mask01: np.ndarray, out_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Save a binary mask (0/1) as PNG (0/255).\n",
    "    \"\"\"\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    m = (mask01.astype(np.uint8) * 255)\n",
    "    cv2.imwrite(str(out_path), m)\n",
    "\n",
    "def parse_filename(fname: str) -> Dict[str, Optional[str]]:\n",
    "    \"\"\"\n",
    "    Parses filenames like: t1.1.8.a.c1.png\n",
    "    Returns dict with channel, section, diameter, trial, modality (c1/c2).\n",
    "\n",
    "    If your naming differs, edit this function.\n",
    "    \"\"\"\n",
    "    stem = Path(fname).name\n",
    "    stem = re.sub(r\"\\.(png|jpg|jpeg|tif|tiff)$\", \"\", stem, flags=re.IGNORECASE)\n",
    "    parts = stem.split(\".\")\n",
    "    out = {\"channel\": None, \"section\": None, \"diameter\": None, \"trial\": None, \"modality\": None}\n",
    "\n",
    "    for token in parts[::-1]:\n",
    "        if token.lower() in (\"c1\", \"c2\"):\n",
    "            out[\"modality\"] = token.lower()\n",
    "            break\n",
    "\n",
    "    if len(parts) >= 5:\n",
    "        out[\"channel\"]  = parts[0]\n",
    "        out[\"section\"]  = parts[1]\n",
    "        out[\"diameter\"] = parts[2]\n",
    "        out[\"trial\"]    = parts[3]\n",
    "    return out\n",
    "\n",
    "def pair_c1_c2(c1_files: List[Path], c2_files: List[Path]) -> List[Tuple[Path, Optional[Path]]]:\n",
    "    \"\"\"\n",
    "    Pair C1 images to their matching C2 image based on filename tokens.\n",
    "    Returns list of (c1, c2_or_None).\n",
    "    \"\"\"\n",
    "    c2_map = {}\n",
    "    for p in c2_files:\n",
    "        stem = p.name.replace(\".c2.\", \".\")\n",
    "        c2_map[stem] = p\n",
    "\n",
    "    pairs = []\n",
    "    for c1 in c1_files:\n",
    "        key = c1.name.replace(\".c1.\", \".\")\n",
    "        pairs.append((c1, c2_map.get(key)))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f437ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# CONFIG (edit these)\n",
    "# =======================\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Folder containing your exported PNGs (will search recursively)\n",
    "INPUT_ROOT = Path(\"data/raw_images\")     # <-- change this\n",
    "OUT_ROOT   = ensure_dir(Path(\"results\"))\n",
    "\n",
    "# Model download (Box)\n",
    "BOX_MODEL_URL = \"https://cornell.box.com/s/4dyu78bhtpabm98jgz40gdp5wmoe71xn?download=1\"\n",
    "MODEL_PATH    = Path(\"models/pillar_unet.pt\")  # saved locally in repo\n",
    "\n",
    "# U-Net input size control\n",
    "SHORT_SIDE = 768  # resize shorter side to this for inference (keeps aspect ratio)\n",
    "\n",
    "# C2 quantification threshold\n",
    "THRESH_METHOD = \"otsu\"   # \"otsu\" or \"fixed\"\n",
    "FIXED_THRESH  = 25       # used only if THRESH_METHOD == \"fixed\" (0..255)\n",
    "\n",
    "# Morphology options for counting/size\n",
    "MIN_COMPONENT_AREA_PX = 10     # ignore tiny components (noise)\n",
    "MORPH_KERNEL = 3               # morphological opening kernel size (px)\n",
    "\n",
    "print(\"INPUT_ROOT:\", INPUT_ROOT.resolve())\n",
    "print(\"OUT_ROOT:\", OUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f26a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Download + load model\n",
    "# =======================\n",
    "\n",
    "if not MODEL_PATH.exists():\n",
    "    print(f\"Model not found at {MODEL_PATH}. Downloading from Box...\")\n",
    "    try:\n",
    "        download_box_file(BOX_MODEL_URL, MODEL_PATH)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            \"Model download failed. If Box requires login or blocks requests, \"\n",
    "            \"manually download the model from the Box link and place it at:\\n\"\n",
    "            f\"  {MODEL_PATH}\\n\\nOriginal error:\\n{e}\"\n",
    "        )\n",
    "\n",
    "print(\"Model file:\", MODEL_PATH.resolve())\n",
    "\n",
    "# Define the exact architecture used in training (edit if needed)\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",\n",
    "    encoder_weights=None,\n",
    "    in_channels=1,\n",
    "    classes=1,\n",
    ")\n",
    "\n",
    "ckpt = torch.load(MODEL_PATH, map_location=\"cpu\")\n",
    "state = ckpt[\"state_dict\"] if (isinstance(ckpt, dict) and \"state_dict\" in ckpt) else ckpt\n",
    "\n",
    "new_state = {}\n",
    "for k, v in state.items():\n",
    "    k2 = k[len(\"model.\"):] if k.startswith(\"model.\") else k\n",
    "    new_state[k2] = v\n",
    "\n",
    "missing, unexpected = model.load_state_dict(new_state, strict=False)\n",
    "print(\"Missing keys:\", missing)\n",
    "print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Discover images\n",
    "# =======================\n",
    "\n",
    "all_imgs = sorted([p for p in INPUT_ROOT.rglob(\"*\") if p.suffix.lower() in (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")])\n",
    "\n",
    "c1_files = [p for p in all_imgs if \".c1.\" in p.name.lower() or p.name.lower().endswith(\".c1.png\")]\n",
    "c2_files = [p for p in all_imgs if \".c2.\" in p.name.lower() or p.name.lower().endswith(\".c2.png\")]\n",
    "\n",
    "print(f\"Found {len(all_imgs)} total images\")\n",
    "print(f\"Found {len(c1_files)} C1 images\")\n",
    "print(f\"Found {len(c2_files)} C2 images\")\n",
    "\n",
    "pairs = pair_c1_c2(c1_files, c2_files)\n",
    "print(\"Pairs:\", len(pairs))\n",
    "\n",
    "MASK_DIR = ensure_dir(OUT_ROOT / \"pillar_masks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0d4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Run pillar inference on C1\n",
    "# =======================\n",
    "\n",
    "def resize_short_side(img: np.ndarray, short_side: int) -> Tuple[np.ndarray, float]:\n",
    "    h, w = img.shape[:2]\n",
    "    scale = short_side / min(h, w)\n",
    "    new_w = int(round(w * scale))\n",
    "    new_h = int(round(h * scale))\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    return resized, scale\n",
    "\n",
    "def infer_pillar_mask(c1_uint8: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Returns binary mask (0/1) same size as input.\"\"\"\n",
    "    img_rs, _ = resize_short_side(c1_uint8, SHORT_SIDE)\n",
    "    x = (img_rs.astype(np.float32) / 255.0)[None, None, :, :]  # (1,1,H,W)\n",
    "    x_t = torch.from_numpy(x).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_t)\n",
    "        prob = torch.sigmoid(logits)[0, 0].detach().cpu().numpy()\n",
    "\n",
    "    prob_up = cv2.resize(prob, (c1_uint8.shape[1], c1_uint8.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "    mask01 = (prob_up >= 0.5).astype(np.uint8)\n",
    "    return mask01\n",
    "\n",
    "mask_index = []\n",
    "\n",
    "for c1_path, c2_path in tqdm(pairs, desc=\"Pillar inference\"):\n",
    "    c1 = read_grayscale_uint8(c1_path)\n",
    "    mask01 = infer_pillar_mask(c1)\n",
    "\n",
    "    rel = c1_path.relative_to(INPUT_ROOT)\n",
    "    out_mask_path = (MASK_DIR / rel).with_suffix(\".mask.png\")\n",
    "    ensure_dir(out_mask_path.parent)\n",
    "    save_mask_png(mask01, out_mask_path)\n",
    "\n",
    "    meta = parse_filename(c1_path.name)\n",
    "    mask_index.append({\n",
    "        \"c1_path\": str(c1_path),\n",
    "        \"c2_path\": str(c2_path) if c2_path is not None else None,\n",
    "        \"mask_path\": str(out_mask_path),\n",
    "        **meta\n",
    "    })\n",
    "\n",
    "mask_index_df = pd.DataFrame(mask_index)\n",
    "mask_index_csv = OUT_ROOT / \"mask_index.csv\"\n",
    "mask_index_df.to_csv(mask_index_csv, index=False)\n",
    "print(\"Saved:\", mask_index_csv)\n",
    "mask_index_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae917012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Quick preview montage (random subset)\n",
    "# =======================\n",
    "\n",
    "N_SHOW = min(12, len(mask_index_df))\n",
    "subset = mask_index_df.sample(N_SHOW, random_state=0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, row in enumerate(subset.itertuples(index=False), start=1):\n",
    "    c1 = read_grayscale_uint8(Path(row.c1_path))\n",
    "    m  = read_grayscale_uint8(Path(row.mask_path))\n",
    "    m01 = (m > 0).astype(np.uint8)\n",
    "\n",
    "    overlay = cv2.cvtColor(c1, cv2.COLOR_GRAY2BGR)\n",
    "    overlay[m01 == 1] = (0, 0, 255)\n",
    "\n",
    "    plt.subplot(3, 4, i)\n",
    "    plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(Path(row.c1_path).name, fontsize=8)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c80546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# C2 quantification (mask out pillars)\n",
    "# =======================\n",
    "\n",
    "def threshold_c2(c2_uint8: np.ndarray, method: str = \"otsu\", fixed_thr: int = 25) -> np.ndarray:\n",
    "    \"\"\"Returns binary (0/1) thresholded C2 image.\"\"\"\n",
    "    if method == \"fixed\":\n",
    "        return (c2_uint8 >= fixed_thr).astype(np.uint8)\n",
    "    elif method == \"otsu\":\n",
    "        _, bw = cv2.threshold(c2_uint8, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        return bw.astype(np.uint8)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'otsu' or 'fixed'\")\n",
    "\n",
    "def component_stats(bw01: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Connected-component stats for a 0/1 binary image.\"\"\"\n",
    "    _, _, stats, _ = cv2.connectedComponentsWithStats(bw01.astype(np.uint8), connectivity=8)\n",
    "    areas = stats[1:, cv2.CC_STAT_AREA].astype(np.float32)  # skip background\n",
    "    areas = areas[areas >= MIN_COMPONENT_AREA_PX]\n",
    "\n",
    "    return {\n",
    "        \"n_components\": int(len(areas)),\n",
    "        \"mean_component_area_px\": float(np.mean(areas)) if len(areas) else 0.0,\n",
    "        \"std_component_area_px\":  float(np.std(areas, ddof=1)) if len(areas) > 1 else 0.0,\n",
    "        \"median_component_area_px\": float(np.median(areas)) if len(areas) else 0.0,\n",
    "    }\n",
    "\n",
    "Q_DIR = ensure_dir(OUT_ROOT / \"quant\")\n",
    "rows = []\n",
    "\n",
    "kernel = np.ones((MORPH_KERNEL, MORPH_KERNEL), np.uint8)\n",
    "\n",
    "for row in tqdm(mask_index_df.itertuples(index=False), total=len(mask_index_df), desc=\"Quantifying C2\"):\n",
    "    if row.c2_path is None:\n",
    "        continue\n",
    "\n",
    "    c2 = read_grayscale_uint8(Path(row.c2_path))\n",
    "    mask = read_grayscale_uint8(Path(row.mask_path))\n",
    "    pillar01 = (mask > 0).astype(np.uint8)\n",
    "\n",
    "    nonpillar01 = (1 - pillar01).astype(np.uint8)\n",
    "    nonpillar_area_px = int(nonpillar01.sum())\n",
    "\n",
    "    bw01 = threshold_c2(c2, method=THRESH_METHOD, fixed_thr=FIXED_THRESH)\n",
    "    crystal01 = (bw01 * nonpillar01).astype(np.uint8)\n",
    "\n",
    "    crystal01_clean = cv2.morphologyEx(crystal01, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    crystal_area_px = int(crystal01_clean.sum())\n",
    "    coverage = (crystal_area_px / nonpillar_area_px) if nonpillar_area_px > 0 else 0.0\n",
    "\n",
    "    comp = component_stats(crystal01_clean)\n",
    "    meta = parse_filename(Path(row.c1_path).name)\n",
    "\n",
    "    rows.append({\n",
    "        \"image_id\": Path(row.c1_path).name.replace(\".c1.\", \".\"),\n",
    "        \"c1_path\": row.c1_path,\n",
    "        \"c2_path\": row.c2_path,\n",
    "        \"mask_path\": row.mask_path,\n",
    "        \"nonpillar_area_px\": nonpillar_area_px,\n",
    "        \"crystal_area_px\": crystal_area_px,\n",
    "        \"coverage_frac\": coverage,\n",
    "        **comp,\n",
    "        **meta\n",
    "    })\n",
    "\n",
    "quant_df = pd.DataFrame(rows)\n",
    "quant_csv = Q_DIR / \"per_image_metrics.csv\"\n",
    "quant_df.to_csv(quant_csv, index=False)\n",
    "print(\"Saved:\", quant_csv)\n",
    "quant_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f548398a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Aggregate by condition\n",
    "# =======================\n",
    "\n",
    "group_cols = [\"channel\", \"section\", \"diameter\", \"trial\"]\n",
    "metric_cols = [\"nonpillar_area_px\", \"crystal_area_px\", \"coverage_frac\", \"n_components\",\n",
    "               \"mean_component_area_px\", \"median_component_area_px\"]\n",
    "\n",
    "agg = (quant_df\n",
    "       .groupby(group_cols, dropna=False)[metric_cols]\n",
    "       .agg([\"mean\", \"std\", \"count\"])\n",
    "       .reset_index())\n",
    "\n",
    "agg.columns = [\"_\".join([c for c in col if c]) for col in agg.columns.to_flat_index()]\n",
    "\n",
    "agg_csv = Q_DIR / \"aggregated_metrics.csv\"\n",
    "agg.to_csv(agg_csv, index=False)\n",
    "print(\"Saved:\", agg_csv)\n",
    "agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb59dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Simple plot\n",
    "# =======================\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "for key, sub in quant_df.groupby([\"channel\", \"section\"], dropna=False):\n",
    "    label = f\"{key[0]} sec{key[1]}\"\n",
    "    x = pd.to_numeric(sub[\"diameter\"], errors=\"coerce\")\n",
    "    plt.scatter(x, sub[\"coverage_frac\"], alpha=0.6, label=label)\n",
    "\n",
    "plt.xlabel(\"Diameter (parsed from filename)\")\n",
    "plt.ylabel(\"Coverage fraction (crystal_area / nonpillar_area)\")\n",
    "plt.title(\"Per-image CaCO₃ coverage (masked)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da89f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======================\n",
    "# Save environment versions\n",
    "# =======================\n",
    "\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version,\n",
    "    \"platform\": platform.platform(),\n",
    "    \"torch\": torch.__version__,\n",
    "    \"segmentation_models_pytorch\": smp.__version__,\n",
    "    \"opencv\": cv2.__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"pandas\": pd.__version__,\n",
    "}\n",
    "\n",
    "env_path = OUT_ROOT / \"environment_versions.json\"\n",
    "with open(env_path, \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", env_path)\n",
    "env"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
